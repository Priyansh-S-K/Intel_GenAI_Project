{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Priyansh-S-K/Intel_GenAI_Project/blob/main/Disease_Symptom_chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUl-rcXcsv-k"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchtext transformers sentencepiece pandas tqdm datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openvino optimum optimum-intel gradio"
      ],
      "metadata": {
        "id": "YhSHV5uCs_XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import pandas as pd\n",
        "import ast\n",
        "import datasets\n",
        "from tqdm import tqdm\n",
        "import time"
      ],
      "metadata": {
        "id": "zPUYIvc-s_Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  data_sample = load_dataset(\"QuyenAnhDE/Diseases_Symptoms\")\n",
        "data_sample = load_dataset(\"keivalya/MedQuad-MedicalQnADataset\")"
      ],
      "metadata": {
        "id": "apYThF_ws_RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "LflxVxyAs_PS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_sample"
      ],
      "metadata": {
        "id": "jRG6ODXhs_MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_data = [{'Question': item['Question'], 'Answer': item['Answer']} for item in data_sample['train']]\n",
        "df = pd.DataFrame(updated_data)\n",
        "\n",
        "# updated_data = [{'Name': item['Name'], 'Treatments': item['Treatments']} for item in data_sample['train']]\n",
        "# df = pd.DataFrame(updated_data)"
      ],
      "metadata": {
        "id": "zXj5_eIjs_Jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.sample(frac=0.4)"
      ],
      "metadata": {
        "id": "t4NWxqq3s_HL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def split_and_join(text):\n",
        "    # Replace multiple delimiters with a space using regex\n",
        "    text = re.sub(r'[ ,:?\"().-]+', ' ', text)\n",
        "    # Split by spaces, strip whitespace, and join with a single space\n",
        "    parts = [part.strip() for part in text.split()]\n",
        "    return ' '.join(parts)"
      ],
      "metadata": {
        "id": "K0cM0zbqs_Ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Question'] = df['Question'].apply(split_and_join)\n",
        "df['Answer'] = df['Answer'].apply(split_and_join)\n",
        "\n",
        "# df['Treatments'] = df['Treatments'].apply(split_and_join)"
      ],
      "metadata": {
        "id": "onREKL8ws_Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split"
      ],
      "metadata": {
        "id": "arr-QvJOs_AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device=torch.device('cpu')"
      ],
      "metadata": {
        "id": "tSBvqftVs-8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "LEj2V7YJs-08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    An extension of the Dataset object to:\n",
        "      - Make training loop cleaner\n",
        "      - Make ingestion easier from pandas df's\n",
        "    \"\"\"\n",
        "    def __init__(self, df, tokenizer):\n",
        "        self.labels = df.columns\n",
        "        self.data = df.to_dict(orient='records')\n",
        "        self.tokenizer = tokenizer\n",
        "        x = self.fittest_max_length(df)  # Fix here\n",
        "        self.max_length = x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx][self.labels[0]]\n",
        "        y = self.data[idx][self.labels[1]]\n",
        "        text = f\"{x} | {y}\"\n",
        "        tokens = self.tokenizer.encode_plus(text, return_tensors='pt', max_length=128, padding='max_length', truncation=True)\n",
        "        return tokens\n",
        "\n",
        "    def fittest_max_length(self, df):  # Fix here\n",
        "        \"\"\"\n",
        "        Smallest power of two larger than the longest term in the data set.\n",
        "        Important to set up max length to speed training time.\n",
        "        \"\"\"\n",
        "        max_length = max(len(max(df[self.labels[0]], key=len)), len(max(df[self.labels[1]], key=len)))\n",
        "        x = 2\n",
        "        while x < max_length: x = x * 2\n",
        "        return x\n",
        "\n",
        "# Cast the Huggingface data set as a LanguageDataset we defined above\n",
        "data_sample = LanguageDataset(df, tokenizer)\n",
        "\n"
      ],
      "metadata": {
        "id": "Us-cQYC2tedC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = int(0.8 * len(data_sample))\n",
        "valid_size = len(data_sample) - train_size\n",
        "train_data, valid_data = random_split(data_sample, [train_size, valid_size])"
      ],
      "metadata": {
        "id": "Ljj3TzbdteZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Make the iterators\n",
        "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "zbKFKWUlteXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1"
      ],
      "metadata": {
        "id": "IVFIxRIwteUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = BATCH_SIZE\n",
        "model_name = 'gpt2'\n",
        "gpu = 0"
      ],
      "metadata": {
        "id": "9GSA9DiYteQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Set the learning rate and loss function\n",
        "## CrossEntropyLoss measures how close answers to the truth.\n",
        "## More punishing for high confidence wrong answers\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tokenizer.pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=5e-4)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "H6QXgj8CteKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Init a results dataframe\n",
        "results = pd.DataFrame(columns=['epoch', 'transformer', 'batch_size', 'gpu',\n",
        "                                'training_loss', 'validation_loss', 'epoch_duration_sec'])"
      ],
      "metadata": {
        "id": "oNntFHR1teAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The training loop\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Start the timer for the epoch\n",
        "\n",
        "    # Training\n",
        "    ## This line tells the model we're in 'learning mode'\n",
        "    model.train()\n",
        "    epoch_training_loss = 0\n",
        "    train_iterator = tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs} Batch Size: {batch_size}, Transformer: {model_name}\")\n",
        "    for batch in train_iterator:\n",
        "        optimizer.zero_grad()\n",
        "        inputs = batch['input_ids'].squeeze(1).to(device)\n",
        "        targets = inputs.clone()\n",
        "        outputs = model(input_ids=inputs, labels=targets)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_iterator.set_postfix({'Training Loss': loss.item()})\n",
        "        epoch_training_loss += loss.item()\n",
        "    avg_epoch_training_loss = epoch_training_loss / len(train_iterator)\n",
        "\n",
        "    # Validation\n",
        "    ## This line below tells the model to 'stop learning'\n",
        "    model.eval()\n",
        "    epoch_validation_loss = 0\n",
        "    total_loss = 0\n",
        "    valid_iterator = tqdm(valid_loader, desc=f\"Validation Epoch {epoch+1}/{num_epochs}\")\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_iterator:\n",
        "            inputs = batch['input_ids'].squeeze(1).to(device)\n",
        "            targets = inputs.clone()\n",
        "            outputs = model(input_ids=inputs, labels=targets)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss\n",
        "            valid_iterator.set_postfix({'Validation Loss': loss.item()})\n",
        "            epoch_validation_loss += loss.item()\n",
        "\n",
        "    avg_epoch_validation_loss = epoch_validation_loss / len(valid_loader)\n",
        "\n",
        "    end_time = time.time()  # End the timer for the epoch\n",
        "    epoch_duration_sec = end_time - start_time  # Calculate the duration in seconds\n",
        "\n",
        "    new_row = {'transformer': model_name,\n",
        "               'batch_size': batch_size,\n",
        "               'gpu': gpu,\n",
        "               'epoch': epoch+1,\n",
        "               'training_loss': avg_epoch_training_loss,\n",
        "               'validation_loss': avg_epoch_validation_loss,\n",
        "               'epoch_duration_sec': epoch_duration_sec}  # Add epoch_duration to the dataframe\n",
        "\n",
        "    results.loc[len(results)] = new_row\n",
        "    print(f\"Epoch: {epoch+1}, Validation Loss: {total_loss/len(valid_loader)}\")\n"
      ],
      "metadata": {
        "id": "YVX-0lsJtuBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/finalmodel\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Colab Notebooks/finalmodel\")"
      ],
      "metadata": {
        "id": "2ptfj97gtt5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from optimum.intel import OVModelForCausalLM\n",
        "from transformers import AutoTokenizer,pipeline\n",
        "model_id=\"/content/drive/MyDrive/Colab Notebooks/finalmodel\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = OVModelForCausalLM.from_pretrained(model_id, export=True)"
      ],
      "metadata": {
        "id": "QSadLeSEttxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import time\n",
        "\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "# Initialize the DistilGPT-2 text generation pipeline\n",
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
        "\n",
        "def gpt2_generate_text(prompt, max_length=100):\n",
        "    # Generate text using the DistilGPT-2 model\n",
        "    generated = generator(prompt, max_length=max_length, num_return_sequences=1)\n",
        "    return generated[0]['generated_text']\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# Medical Chatbot\")\n",
        "    gr.Markdown(\"### Example questions to ask: 'Panic Disorder', 'Turner Syndrome', 'Vocal cord polyp', 'Cryptochidism\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    msg = gr.Textbox()\n",
        "    submit=gr.Button(\"Submit\")\n",
        "    clear = gr.Button(\"Clear\")\n",
        "\n",
        "\n",
        "\n",
        "    def user(user_message, history):\n",
        "        return \"\", history + [[user_message, None]]\n",
        "\n",
        "    def bot(history):\n",
        "        bot_message = gpt2_generate_text(history[-1][0])\n",
        "        history[-1][1] = \"\"\n",
        "        for character in bot_message:\n",
        "            history[-1][1] += character\n",
        "            time.sleep(0.05)\n",
        "            yield history\n",
        "\n",
        "    submit.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
        "        bot, chatbot, chatbot\n",
        "    )\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)\n",
        "\n",
        "demo.queue()\n",
        "demo.launch()\n"
      ],
      "metadata": {
        "id": "NV2tZRxSuKfI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}